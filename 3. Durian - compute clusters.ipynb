{
 "cells": [
  {
   "source": [
    "# Build an image classifier that tells if a fruit is a Durian or a Jackfruit\n",
    "---\n",
    "### Key Concepts:\n",
    "  * Image Classifier (2 classes: {Durian, Jackfruit})\n",
    "  * Transfer Learning (ResNet18)\n",
    "  * Pytorch\n",
    "  * ** <u>REMOTE, distributed compute and data</u> **\n",
    "  * ** <u>Azure Machine Learning SDK</u> **\n",
    "\n",
    "### Learning Objectives:\n",
    "  * Load and explore Image dataset\n",
    "  * Train model\n",
    "  * Predict based on model (Inference)\n",
    "    * ** <u>Register model</u> **\n",
    "    * ** <u>Build and invoke webservice</u> **\n",
    "\n",
    "###  Dataset:\n",
    "  * small dataset containing images of Durians and Jackfruits\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "source": [
    "experiment_name = 'DurianvsJackfruit_Experiment'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the compute target\n",
    "compute_target = \"cc-pycon2020\" # REMOTE CLUSTER\n",
    "\n",
    "# Traning variables\n",
    "n_epochs = 2\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "folder_name = 'durian-experiment-files'\n",
    "experiment_folder = './' + folder_name\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder reference')\n",
    "parser.add_argument('--epochs', type=str, dest='epochs', default=\"1\", help='number of epochs')\n",
    "parser.add_argument('--learning-rate', type=float, dest='l_rate', default=0.005, help='learning rate')\n",
    "parser.add_argument('--output_folder', type=str, dest='output_folder', default=\"outputs\", help='output folder')\n",
    "\n",
    "args = parser.parse_args()\n",
    "data_folder = args.data_folder\n",
    "epochs = int(args.epochs)\n",
    "lrate = args.l_rate\n",
    "output_folder = args.output_folder\n",
    "\n",
    "data_dir = data_folder\n",
    "print('Mounting at: ',data_dir)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ',device)\n",
    "print('Epochs:',str(epochs))\n",
    "print('Learning Rate',lrate)\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1 \n",
    "#number of colors on images (B&W=1, RGB=3)\n",
    "num_channels = 3\n",
    "model_save_path = 'durian.pth'\n",
    "\n",
    "#\n",
    "# Do Data Transformations and load into Dataloader\n",
    "#\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([256, 256]),        \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([256, 256]),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([256, 256]),\n",
    "        transforms.ToTensor(),\n",
    "    ])   \n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                        data_transforms[x])\n",
    "                for x in ['train', 'val','test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "            for x in ['train', 'val','test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print('Classes loaded:',class_names)\n",
    "\n",
    "num_letters = len(class_names)\n",
    "\n",
    "def train_model(net, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    lowest_loss = 100.0\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.7f} Acc: {:.7f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "            if phase == 'val' and epoch_loss < lowest_loss:\n",
    "                lowest_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest val loss: {:4f}'.format(lowest_loss))\n",
    "\n",
    "    print('accuracy: ' + str(best_acc))\n",
    "    run.log('accuracy', float(best_acc.cpu().numpy()))\n",
    "            \n",
    "    print('Loss: ' + str(lowest_loss))\n",
    "    run.log('loss', float(lowest_loss))        \n",
    "        \n",
    "    print('Finished Training')\n",
    "    return net\n",
    "    \n",
    "run = Run.get_context()\n",
    "\n",
    "print('Started training')\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "print('Number of classes:',len(class_names))\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=lrate, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "net = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=epochs)\n",
    "\n",
    "print('Model trained.')\n",
    "\n",
    "\n",
    "# saving models\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "torch.save(net, output_folder+'/'+model_save_path)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore,Dataset\n",
    "\n",
    "datastore = Datastore.get(ws, 'pycon2020_source_images')\n",
    "planes_ds = Dataset.File.from_files((datastore, 'durian/*/*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "# Set up the parameters\n",
    "script_params = {\n",
    "    '--data-folder': planes_ds.as_named_input('pycon2020_source_images').as_mount(),\n",
    "    '--epochs': n_epochs,\n",
    "    '--learning-rate': learning_rate\n",
    "}\n",
    "\n",
    "# Create an estimator\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(source_directory=experiment_folder,\n",
    "        script_params=script_params,\n",
    "        compute_target=compute_target,\n",
    "        entry_script='train.py',\n",
    "        use_gpu=True,\n",
    "        pip_packages=['azureml-dataprep[pandas,fuse]'],\n",
    "        conda_packages=['pytorch','torchvision'])\n",
    "\n",
    "# Run the experiment\n",
    "run = exp.submit(config=estimator)\n",
    "\n",
    "# Show the run details while running\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model with AML Service\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_path = 'durian.pth'\n",
    "\n",
    "# register model \n",
    "model = run.register_model(model_name='durian', model_path='outputs/durian.pth')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "---\n",
    "1. Create environment dependencies\n",
    "1. Build score.py script (that get's executed on the Webservice calls)\n",
    "1. Deploy Webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=['azureml-defaults', 'torch', 'torchvision>=0.5.0'])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    \n",
    "print(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'durian.pth')\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    input_data = torch.tensor(json.loads(input_data)['data'])\n",
    "\n",
    "    # get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "        classes = ['Durian', 'Jackfruit']\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        pred_probs = softmax(output).numpy()[0]\n",
    "        index = torch.argmax(output, 1)\n",
    "\n",
    "    result = {\"label\": classes[index], \"probability\": str(pred_probs[index])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'data': 'Durians and Jackfruits',  \n",
    "                                                     'method':'transfer learning', 'framework':'pytorch'},\n",
    "                                               description='Classify Durians/Jackfruits using transfer learning with PyTorch')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='aci-durian', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke scoring Web Service\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "# import json\n",
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "    \n",
    "def preprocess(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize([256, 256]),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.numpy()\n",
    "\n",
    "def load_image():\n",
    "    # ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    URL = 'https://upload.wikimedia.org/wikipedia/commons/b/bc/Durian_in_black.jpg'\n",
    "\n",
    "    request = urllib.request.Request(URL)\n",
    "    image = urllib.request.urlopen(request)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = load_image()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(Image.open(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "import json\n",
    "\n",
    "# Print webservices in this workspace\n",
    "services = Webservice.list(ws)\n",
    "\n",
    "for service in services:\n",
    "    print(service.name)\n",
    "    print(service.scoring_uri)\n",
    "    print(service.swagger_uri)\n",
    "\n",
    "# Get our Webservice (by name)\n",
    "service = Webservice(workspace=ws,name='aci-durian')\n",
    "\n",
    "# Call the webservice and print results for the image in URL\n",
    "image = load_image()\n",
    "input_data = preprocess(image)\n",
    "\n",
    "result = service.run(input_data=json.dumps({'data': input_data.tolist()}))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = 'http://bc33c818-47c7-441e-a78d-02b68857d452.southeastasia.azurecontainer.io/score'\n",
    "\n",
    "headers = {'Content-type':'application/json'}\n",
    "\n",
    "image = load_image()\n",
    "input_data = preprocess(image)\n",
    "\n",
    "response = requests.post(scoring_uri, data = json.dumps({'data': input_data.tolist()}), headers = headers)\n",
    "\n",
    "print(response.json())\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}